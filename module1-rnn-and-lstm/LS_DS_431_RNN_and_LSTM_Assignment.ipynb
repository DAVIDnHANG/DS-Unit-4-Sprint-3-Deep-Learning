{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, \n",
    "and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - \n",
    "a function that can take, as an argument, the size of text (e.g. number of characters or lines) \n",
    "to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, \n",
    "to sample/use smaller data and parameters, so you can have a tighter feedback \n",
    "loop when you're trying to get things running. Then, once you've got a proof of concept - \n",
    "start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Tokenizer\n",
    "abstracts[100][:300]\n",
    "#create tokenizer object\n",
    "tokenizer = Tokenizer(num_words = None, filters = '!\"#$%&()/+*-,./:;<=>?@[\\\\]^_{|}~\\t\\n', lower=True, split='')\n",
    "#train tokenizer ot the texts\n",
    "tokenizer.fit_on_texts(abstracts)\n",
    "#conert list of string into list of list of integer\n",
    "sequences = tokenizer.texts_to_sequence(abstracts)\n",
    "sequences[100][:15]\n",
    "#mapping of indexes to words\n",
    "idx_word=tokenizer.index_word\n",
    "' '.join(idx_word[w] for w in sequences[100][:40])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features =[]\n",
    "labels = []\n",
    "training_length = 50\n",
    "#iterate through the sequences of tokens\n",
    "for seq in sequences:\n",
    "    #create multiple training example frome ach seqences\n",
    "    for i in range(training_length, len(seq)):\n",
    "        #extract the features and label\n",
    "        extract = seq[i - training_length:i +1]\n",
    "        #set the feature and label\n",
    "        features.append(extract[:-1])\n",
    "        labels.append(extract[-1])\n",
    "features = np.array(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_words = len(word_idx) + 1\n",
    "#empty array to hold labels\n",
    "label_array  =np.zeros((len(features), num_words), dtype=np.int8)\n",
    "#1 hot encode the label\n",
    "for examle_index, word_index in enumerate(labels):\n",
    "    label_array[example_index, word_index] = 1\n",
    "label_array.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#embed layer\n",
    "model.add(\n",
    "    Embedding(input_dim=num_words,\n",
    "              input_length = training_length,\n",
    "              output_dim=100,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True)\n",
    ")\n",
    "#masking layer for pre_train\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "#recurrent layer\n",
    "model.add(LSTM(64, return_sequence=False, dropout=.1, recurrent_dropout=.1))\n",
    "\n",
    "#fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "#dropout for regularization\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_vector = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "glove = np.loadtxt(glove_vector, dtype='str', comments = None)\n",
    "#extract the vector and words\n",
    "vectors = glove[:,1:].astype('float')\n",
    "words=glove[:,0]\n",
    "#create lookup of words to vectors\n",
    "word_lookup = {word:vector for word, vector in zip(words,vectors)}\n",
    "#new matrix to hold word embedding\n",
    "embedding_matrix = np.zeros((num_words, vector.shape[1]))\n",
    "for i, word in enumerate(word_idx.keys()):\n",
    "    #look up word embedding\n",
    "    vector = word_lookup.get(word,None)\n",
    "    \n",
    "    #record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i+1, :] = vector\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% pretrain embed\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_lookup['neural'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#create callbacks\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5), ModelCheckpoint('../models/model.h5'), save_best_only=True, save_weights_only=False]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=2048, epochs=150, callbacks=callbacks, validation_data=(X_valid,y_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras import load_model\n",
    "model = load_model('../models/model.h5')\n",
    "model.evaluate(X_valid,y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-7bde015050c1>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    def step(self, x):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ],
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-7bde015050c1>, line 4)",
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class RNN:\n",
    "    #\n",
    "def step(self, x):\n",
    "    #update the hidden state\n",
    "    #W_hh matrix\n",
    "    #W_xh matrix\n",
    "    #w_hy matrix\n",
    "    #tanh - based previous hidden state, one is current input. keep between [-1,1]\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    #compute the output vector\n",
    "    y = np.dot(self.Why, self.h)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1910241558e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# TODO - Words, words, mere words, no matter from the heart.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RNN' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "y = rnn.step(x) # x is an input vector, y is the RNN output vector\n",
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "y1 = rnn2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% let make a tokenizer\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}